
#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil expand-links:t f:t
#+options: inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: SA-Toolkit
#+date: <2025-12-16 Mon>
#+author: David A. Ventimiglia
#+email: david.ventimiglia@supabase.io
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 30.1 (Org mode 9.7.11)
#+cite_export:

#+STARTUP: indent

* What

This is a Solutions Architect Toolkitâ€”a collection of tools for PostgreSQL diagnostics and troubleshooting.

* Why

When batch jobs exhibit variance (e.g., usually 10 minutes but sometimes 60 minutes), you need telemetry to answer: "Why did this batch take so long?"

* Tools

** batch-telemetry/sql/

Server-side batch telemetry for PostgreSQL 15, 16, or 17. Automatically collects performance metrics via ~pg_cron~ to diagnose batch job variance.

*** Requirements

- PostgreSQL 15, 16, or 17
- ~pg_cron~ extension (1.4.1+ recommended for 30-second sampling)

*** Installation

#+begin_src bash
psql -f batch-telemetry/sql/install.sql
#+end_src

*** Uninstall

#+begin_src bash
psql -f batch-telemetry/sql/uninstall.sql
#+end_src

*** Quick Start

#+begin_src sql
-- 1. Track your target table(s) before running the batch
SELECT telemetry.track_table('orders');

-- 2. Run your batch job, note the start/end times
--    (telemetry collects automatically in the background)

-- 3. Analyze the batch window
SELECT * FROM telemetry.compare('2024-12-16 14:00', '2024-12-16 15:00');
SELECT * FROM telemetry.table_compare('orders', '2024-12-16 14:00', '2024-12-16 15:00');
SELECT * FROM telemetry.wait_summary('2024-12-16 14:00', '2024-12-16 15:00');
#+end_src

*** Two-Tier Collection

Telemetry runs automatically via ~pg_cron~:

| Tier      | Interval   | What's Captured                                          |
|-----------+------------+----------------------------------------------------------|
| Snapshots | 5 minutes  | WAL, checkpoints, bgwriter, replication, temp files, I/O |
| Samples   | 30 seconds | Wait events, active sessions, locks, operation progress  |

*** Key Functions

| Function                                    | Purpose                                   |
|---------------------------------------------+-------------------------------------------|
| ~telemetry.track_table(name, schema)~       | Register a table for monitoring           |
| ~telemetry.compare(start, end)~             | Compare system stats between time points  |
| ~telemetry.table_compare(table, start, end)~ | Compare table stats between time points   |
| ~telemetry.wait_summary(start, end)~        | Aggregate wait events over a time period  |
| ~telemetry.cleanup(interval)~               | Remove old data (default: retain 7 days)  |

*** Key Views

| View                          | Purpose                                    |
|-------------------------------+--------------------------------------------|
| ~telemetry.deltas~            | Snapshot deltas (checkpoint, WAL, buffers) |
| ~telemetry.table_deltas~      | Per-table deltas (size, tuples, vacuum)    |
| ~telemetry.recent_locks~      | Lock contention (last 2 hours)             |
| ~telemetry.recent_waits~      | Wait events (last 2 hours)                 |
| ~telemetry.recent_progress~   | Vacuum/COPY/analyze progress (last 2 hours)|
| ~telemetry.recent_replication~| Replication lag (last 2 hours)             |

*** Diagnostic Patterns

**** Pattern 1: Lock Contention

Symptoms:
- Batch takes 10x longer than expected
- ~telemetry.recent_locks~ shows ~blocked_pid~ entries

#+begin_src sql
SELECT * FROM telemetry.recent_locks
WHERE captured_at BETWEEN '2024-12-16 14:00' AND '2024-12-16 15:00';
#+end_src

**** Pattern 2: Buffer Pressure

Symptoms:
- ~bgw_buffers_backend_delta > 0~ (backends writing directly)
- Wait events: ~LWLock:WALWrite~, ~Lock:extend~

#+begin_src sql
SELECT bgw_buffers_backend_delta, wal_bytes_pretty
FROM telemetry.compare('2024-12-16 14:00', '2024-12-16 15:00');
#+end_src

**** Pattern 3: Checkpoint Interference

Symptoms:
- ~checkpoint_occurred = true~ during batch
- High ~ckpt_write_time_ms~

**** Pattern 4: Autovacuum Interference

Symptoms:
- ~table_compare()~ shows ~autovacuum_ran = true~
- ~recent_progress~ shows vacuum overlapping batch

**** Pattern 5: Temp File Spills

Symptoms:
- ~temp_files_delta > 0~
- Large ~temp_bytes_delta~ (work_mem exhaustion)

Resolution: Increase ~work_mem~ for the session.

**** Pattern 6: Replication Lag

Symptoms:
- ~recent_replication~ shows large ~replay_lag_bytes~
- Batch runs slowly despite no local contention

Resolution: Check replica health or switch to async replication.

*** Quick Diagnosis Checklist

For a slow batch between ~START~ and ~END~:

#+begin_src sql
-- 1. Overall health
SELECT * FROM telemetry.compare('START', 'END');

-- 2. Lock contention
SELECT * FROM telemetry.recent_locks
WHERE captured_at BETWEEN 'START' AND 'END';

-- 3. Wait events
SELECT * FROM telemetry.wait_summary('START', 'END');

-- 4. Table-specific (if tracking)
SELECT * FROM telemetry.table_compare('mytable', 'START', 'END');

-- 5. Active operations
SELECT * FROM telemetry.recent_progress
WHERE captured_at BETWEEN 'START' AND 'END';

-- 6. Replication lag
SELECT * FROM telemetry.recent_replication
WHERE captured_at BETWEEN 'START' AND 'END';
#+end_src

*** PG Version Differences

| Version | Checkpoint Stats         | pg_stat_io |
|---------+--------------------------+------------|
| PG 15   | ~pg_stat_bgwriter~       | No         |
| PG 16   | ~pg_stat_bgwriter~       | Yes        |
| PG 17   | ~pg_stat_checkpointer~   | Yes        |
